{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW6TGEyznNqu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kt471\\Anaconda3\\envs\\clean_rl\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "c:\\Users\\kt471\\Anaconda3\\envs\\clean_rl\\lib\\site-packages\\botocore\\httpsession.py:41: DeprecationWarning: 'urllib3.contrib.pyopenssl' module is deprecated and will be removed in a future release of urllib3 2.x. Read more in this issue: https://github.com/urllib3/urllib3/issues/2680\n",
      "  from urllib3.contrib.pyopenssl import orig_util_SSLContext as SSLContext\n"
     ]
    }
   ],
   "source": [
    "# Importing the required library\n",
    "import sys\n",
    "import os \n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time \n",
    "import argparse\n",
    "sys.argv = ['']\n",
    "del sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-v_1\", \"--verbose_1\", help=\" \",\n",
    "                    action=\"store_true\", default='Original+linear layer to see if Q is negative ' )\n",
    "parser.add_argument(\"-seed\", \"--seed\", help=\"increase output verbosity\",\n",
    "                    action=\"store_true\",default = 1)\n",
    "parser.add_argument(\"-NUMBER_OF_EPISODES\", \"--NUMBER_OF_EPISODES\", help=\"Total_Episodes to run \",\n",
    "                    action=\"store_true\",default = 200)\n",
    "parser.add_argument(\"-MAX_STEPS\", \"--MAX_STEPS\", help=\"MAX_STEPS to run each episode \",\n",
    "                    action=\"store_true\",default = 1369)\n",
    "parser.add_argument(\"-LEARNING_RATE\", \"--LEARNING_RATE\", help=\"LEARNING_RATE \",\n",
    "                    action=\"store_true\",default = 0.0001) #0.0001\n",
    "\n",
    "parser.add_argument(\"-UCB_LEARNING_RATE\", \"--UCB_LEARNING_RATE\", help=\"UCB_LEARNING_RATE \",\n",
    "                    action=\"store_true\",default = 0.0001) #0.0001\n",
    "\n",
    "parser.add_argument(\"-UCB_FILTER\", \"--UCB_FILTER\", help=\"UCB_FILTER \",\n",
    "                    action=\"store_true\",default = 0.0001)\n",
    "\n",
    "parser.add_argument(\"-DISCOUNT_FACTOR\", \"--DISCOUNT_FACTOR\", help=\"DISCOUNT_FACTOR \",\n",
    "                    action=\"store_true\",default = 0.99)\n",
    "\n",
    "parser.add_argument(\"-HIDDEN_LAYER_SIZE\", \"--HIDDEN_LAYER_SIZE\", help=\"HIDDEN_LAYER_SIZE \", \n",
    "                    action=\"store_true\",default = 128)\n",
    "\n",
    "parser.add_argument(\"-UCB_HIDDEN_LAYER_SIZE\", \"--UCB_HIDDEN_LAYER_SIZE\", help=\"UCB HIDDEN_LAYER_SIZE \",\n",
    "                    action=\"store_true\", default=128)\n",
    "\n",
    "parser.add_argument(\"-EGREEDY_START\", \"--EGREEDY_START\", help=\"EGREEDY_START \",\n",
    "                    action=\"store_true\",default = 0.4) #0.4\n",
    "parser.add_argument(\"-EGREEDY_FINAL\", \"--EGREEDY_FINAL\", help=\"EGREEDY_FINAL \",\n",
    "                    action=\"store_true\",default = 0.01) #0.01\n",
    "parser.add_argument(\"-EGREEDY_DECAY\", \"--EGREEDY_DECAY\", help=\"EGREEDY_DECAY- not being used \",\n",
    "                    action=\"store_true\",default = 2000)\n",
    "parser.add_argument(\"-REPLAY_BUFFER_SIZE\", \"--REPLAY_BUFFER_SIZE\", help=\"REPLAY_BUFFER_SIZE \",\n",
    "                    action=\"store_true\",default = 40000)\n",
    "parser.add_argument(\"-BATCH_SIZE\", \"--BATCH_SIZE\", help=\"BATCH_SIZE \",\n",
    "                    action=\"store_true\",default = 64)\n",
    "parser.add_argument(\"-UPDATE_TARGET_FREQUENCY\", \"--UPDATE_TARGET_FREQUENCY\", help=\"UPDATE_TARGET_FREQUENCY \",\n",
    "                    action=\"store_true\", default=2000)\n",
    "parser.add_argument(\"-TEST_MODEL_FREQ\", \"--TEST_MODEL_FREQ\", type=int, help=\"After How many episodes you want to test the model \",\n",
    "                     default=1)\n",
    "\n",
    "parser.add_argument(\"-TEST_RUN_EPISODE\", \"-TEST_RUN_EPISODE\", help=\" Number of episodes to run while testing . \",\n",
    "                    action=\"store_true\", default=10)\n",
    "\n",
    "\n",
    "parser.add_argument(\"-MODEL_SAVE_FREQUENCY\", \"--MODEL_SAVE_FREQUENCY\", help=\"After How many episodes you want to save the model \",\n",
    "                    action=\"store_true\", default=10)\n",
    "\n",
    "parser.add_argument(\"-TRAINING_STARTS\", \"--TRAINING_STARTS\", type=int, help=\"After How many episodes does the optimization starts,currently 10 cycles. \",\n",
    "                     default=0)\n",
    "\n",
    "parser.add_argument(\"-TAU\", \"--TAU\", type=int, help=\"Target Network Update.,tau = 0 no update , tau = 1 imediate update of target with latest, new_target = q * tau + (1-tau) * target \",\n",
    "                     default=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"-run_name\", \"--run_name\", help=\"run name\",\n",
    "                    action=\"store_true\", default=f\"{'_vehcile_13_random_initial_SOC'}_{int(time.time())}\")\n",
    "args = parser.parse_args()\n",
    "# args = argparse.Namespace(verbose=False, verbose_1=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.TEST_MODEL_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{'DDQN'}{args.run_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5052,
     "status": "ok",
     "timestamp": 1586934273584,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "4WvcVL8VnNrQ",
    "outputId": "64eee3ac-d95f-4645-eee5-ffe9d8f13eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# selecting the available device (cpu/gpu)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWZft9z6nNrm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial soc is 0.3\n",
      "331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.40000919697613346"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the CartPole-v0 environment\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "env = gym.make('Vehicle-v0') \n",
    "env.reset()\n",
    "# Creating the CartPole-v0 environment\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('Vehicle-fixedSOC')\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "new_state, reward, done, info = env.step(action)\n",
    "# new_state[2]\n",
    "print(action)\n",
    "new_state[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2v8unZtnNrz"
   },
   "outputs": [],
   "source": [
    "# setting the seed value for reproducibility\n",
    "\n",
    "# seed = 1626\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "# run_name = f\"{seed}__{int(time.time())}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6627,
     "status": "ok",
     "timestamp": 1586934275172,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "79XQqTKSnNr8",
    "outputId": "f0ef9d80-1b41-4ba6-ff58-2c3e214d8629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of States : 3\n",
      "Total number of Actions : 2000\n"
     ]
    }
   ],
   "source": [
    "# Fetching the number of states and actions\n",
    "#number_of_states = env.observation_space.n\n",
    "number_of_states = env.observation_space.shape[0]\n",
    "number_of_actions = env.action_space.n\n",
    "# checking the total number of states and action\n",
    "print('Total number of States : {}'.format(number_of_states)) \n",
    "print('Total number of Actions : {}'.format(number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.Sequential(\n",
    "#     nn.Linear(1, 1, bias=False),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(1, 1, bias=False)\n",
    "# )\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# x = torch.randn(1, 1)\n",
    "# target = torch.ones(1, 1)\n",
    "\n",
    "# output = model(x)\n",
    "# loss = criterion(output, target)\n",
    "# loss.backward()\n",
    "\n",
    "# lin1_grad = model[0].weight.grad.clone()\n",
    "# lin2_grad = model[2].weight.grad.clone()\n",
    "# print('Before regularization')\n",
    "# print('Grad lin1: {}'.format(lin1_grad))\n",
    "# print('Grad lin2: {}'.format(lin2_grad))\n",
    "\n",
    "# # Add regularization to lin1\n",
    "# model.zero_grad()\n",
    "# output = model(x)\n",
    "# loss = criterion(output, target)\n",
    "# loss = loss + torch.norm(model[0].weight)\n",
    "# loss.backward()\n",
    "\n",
    "# lin1_grad_reg = model[0].weight.grad.clone()\n",
    "# lin2_grad_reg = model[2].weight.grad.clone()\n",
    "# print('After regularization')\n",
    "# print('Grad lin1: {}'.format(lin1_grad_reg))\n",
    "# print('Grad lin2: {}'.format(lin2_grad_reg))\n",
    "# print('As you can see, the gradient for lin2 stays the same, while the gradient for lin1 changes.This is due to the fact, that the parameters of lin2 were not involved in creating the regularization term, so they won’t be touched.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szLMi9vUnNsF"
   },
   "outputs": [],
   "source": [
    "# # Setting the Hyper parameter Values for Q Learning\n",
    "\n",
    "# NUMBER_OF_EPISODES = 4000\n",
    "# MAX_STEPS = 1369\n",
    "# LEARNING_RATE = 0.001\n",
    "# DISCOUNT_FACTOR = 0.99\n",
    "# HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "# EGREEDY_START = 0.9\n",
    "# EGREEDY_FINAL = 0.002\n",
    "# EGREEDY_DECAY = 2200\n",
    "\n",
    "# REPLAY_BUFFER_SIZE = 20000\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# UPDATE_TARGET_FREQUENCY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(EGREEDY: float, EGREEDY_FINAL: float, duration: int, t: int):\n",
    "    slope = (EGREEDY_FINAL - EGREEDY) / duration\n",
    "    return max(slope * t + EGREEDY, EGREEDY_FINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8EJlCeVnNsM"
   },
   "outputs": [],
   "source": [
    "# def calculate_epsilon(steps_done):\n",
    "#     \"\"\"\n",
    "#     Decays eplison with increasing steps\n",
    "#     Parameter:\n",
    "#     steps_done (int) : number of steps completed\n",
    "#     Returns:\n",
    "#     int - decayed epsilon\n",
    "#     \"\"\"\n",
    "#     epsilon = EGREEDY_FINAL + (EGREEDY - EGREEDY_FINAL) * \\\n",
    "#               math.exp(-1. * steps_done / EGREEDY_DECAY )\n",
    "#     return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqEC_YRRnNsS"
   },
   "outputs": [],
   "source": [
    "# Deep Q Network Model Architecture\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self , hidden_layer_size):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(args.seed)\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        # self.fc0 = nn.Linear(number_of_states,self.hidden_layer_size)\n",
    "        self.fc1 = nn.Linear(number_of_states, self.hidden_layer_size)\n",
    "        # self.fc2 = nn.Linear( self.hidden_layer_size, self.hidden_layer_size)\n",
    "        # self.fc3 = nn.Linear( self.hidden_layer_size, self.hidden_layer_size)\n",
    "        # self.fc4 = nn.Linear( self.hidden_layer_size, self.hidden_layer_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_layer_size,number_of_actions)\n",
    "        \n",
    "    # def _init_weights(self, module):\n",
    "    #     if isinstance(module, nn.Linear):\n",
    "    #         module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "    #         if module.bias is not None:\n",
    "    #             module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        output = torch.relu(self.fc1(x))\n",
    "        # output = self.fc1(x)\n",
    "        # output = torch.relu(self.fc2(output))\n",
    "        # output = torch.relu(self.fc3(output))\n",
    "        # output = torch.relu(self.fc4(output))\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([-100,180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmaxnorm(x,Min,Max):\n",
    "    meanX = np.mean([Min,Max])\n",
    "    x_out = 2*(x - meanX)/(Max-Min)\n",
    "    return(x_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmaxnorm(0,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLrMhP3UnNsW"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self , capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def push(self , state, action, new_state, reward, done):\n",
    "        experience = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.pointer >= len(self.buffer):\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.pointer] = experience\n",
    "        \n",
    "        self.pointer = (self.pointer + 1) % self.capacity\n",
    "        \n",
    "    def sample(self , batch_size):\n",
    "        # random.seed(args.seed)\n",
    "        return zip(*random.sample(self.buffer , batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoqCEYxnnNsc"
   },
   "outputs": [],
   "source": [
    "# Instantiating the ExperienceReplay\n",
    "memory = ExperienceReplay(args.REPLAY_BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZO2ic3VVnNsi"
   },
   "outputs": [],
   "source": [
    "# Building the brain of the network i.e. the DQN Agent\n",
    "\n",
    "class DQN_Agent(object):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(args.seed)\n",
    "        self.dqn = DQN(args.HIDDEN_LAYER_SIZE).to(device)\n",
    "        self.target_dqn = DQN(args.HIDDEN_LAYER_SIZE).to(device)\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "        \n",
    "        # self.criterion = torch.nn.MSELoss()\n",
    "        self.criterion = torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=5.0)\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.dqn.parameters(), lr=args.LEARNING_RATE)\n",
    "        \n",
    "        self.target_dqn_update_counter = 0\n",
    "        self.best_reward = -99999\n",
    "        self.prev_best_episode = 0\n",
    "    \n",
    "    def select_action(self, state, EGREEDY):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > EGREEDY:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = torch.Tensor(state).to(device)\n",
    "                q_values = self.dqn(state) # + round(random.uniform(0.1, 5), 1)\n",
    "                action = torch.max(q_values,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            # random.seed(args.seed)\n",
    "            action = env.action_space.sample()\n",
    "            # action = 1998\n",
    "            # print(action)\n",
    "            \n",
    "        # print(action)\n",
    "        return action\n",
    "    \n",
    "    def optimize(self,epi,LR):\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.dqn.parameters(), lr=LR)\n",
    "        \n",
    "        if (args.BATCH_SIZE > len(memory)):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(args.BATCH_SIZE)\n",
    "        # print(reward)\n",
    "        \n",
    "        state = torch.Tensor(state).to(device)\n",
    "        new_state = torch.Tensor(new_state).to(device)\n",
    "        reward = torch.Tensor(reward).to(device)\n",
    "        action = torch.LongTensor(action).to(device)\n",
    "        done = torch.Tensor(done).to(device)\n",
    "        \n",
    "        # select action : get the index associated with max q value from prediction network\n",
    "        new_state_indxs = self.dqn(new_state).detach() \n",
    "        max_new_state_indxs = torch.max(new_state_indxs, 1)[1] # to get the max new state indexes\n",
    "        # print(max_new_state_indxs)\n",
    "        \n",
    "        \n",
    "        # Using the best action from the prediction nn get the max new state value in target dqn\n",
    "        new_state_values = self.target_dqn(new_state).detach()\n",
    "        max_new_state_values = new_state_values.gather(1, max_new_state_indxs.unsqueeze(1)).squeeze(1)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        target_value = reward + (1 - done) * args.DISCOUNT_FACTOR * max_new_state_values #when done = 1 then target = reward\n",
    "        \n",
    "        predicted_value = self.dqn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print()\n",
    "        # print(predicted_value,target_value)\n",
    "        loss = self.criterion(predicted_value, target_value)\n",
    "        # loss = self.criterion(target_value, predicted_value)\n",
    "        loss_output = self.criterion(predicted_value, target_value)\n",
    "        # loss_output = self.criterion(target_value, predicted_value)\n",
    "        loss_output_diff = predicted_value - target_value\n",
    "        # loss_output_diff = target_value - predicted_value\n",
    "        # print('loss',loss.mean().item())\n",
    "        writer.add_scalar(\"train/losses/td_loss_steps_abs\", loss_output_diff.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/td_loss_steps\", loss.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/predictedmean\", predicted_value.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/targetmean\", target_value.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/predictedmin\", predicted_value.min().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/targetmin\", target_value.min().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/predictedmax\",\n",
    "                          predicted_value.max().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/targetmax\", target_value.max().item(), epi)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # for param in self.dqn.parameters():\n",
    "        #     param.grad.data.clamp_(-1, 1)\n",
    "        for tag, value in self.dqn.named_parameters():\n",
    "            writer.add_scalar(\"train/charts/gradsum \",\n",
    "                            np.array(value.grad.cpu()).sum(), epi)\n",
    "            writer.add_scalar(\"train/charts/gradmean\", np.array(value.grad.cpu()).mean(), epi)\n",
    "            writer.add_scalar(\"train/charts/gradmin\", np.array(value.grad.cpu()).min(), epi)\n",
    "            writer.add_scalar(\"train/charts/gradmax\", np.array(value.grad.cpu()).max(), epi)\n",
    "        self.optimizer.step()\n",
    "        # torch.nn.utils.clip_grad_norm(\n",
    "        #     parameters=self.dqn.parameters(), max_norm=1, norm_type=2.0)\n",
    "        \n",
    "        if (self.target_dqn_update_counter % args.UPDATE_TARGET_FREQUENCY == 0):\n",
    "            # self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "            ## new\n",
    "            target_net_state_dict = self.target_dqn.state_dict()\n",
    "            policy_net_state_dict = self.dqn.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key] * \\\n",
    "                args.TAU + target_net_state_dict[key]*(1-args.TAU)\n",
    "            self.target_dqn.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        #     for target_network_param, q_network_param in zip(self.target_dqn.parameters(), self.dqn.parameters()):\n",
    "        #         target_network_param.data.copy_(\n",
    "        #             0.5 * q_network_param.data +\n",
    "        #             (1.0 - 0.5) * target_network_param.data\n",
    "        #         )\n",
    "        #     self.target_dqn.load_state_dict(target_network_param.data.state_dict())\n",
    "        # self.target_dqn_update_counter += 1\n",
    "\n",
    "            \n",
    "        return (loss.mean().item())\n",
    "        \n",
    "    def save_model(self,epi,total_epi):\n",
    "        if (epi % args.MODEL_SAVE_FREQUENCY ==0):\n",
    "            torch.save(self.dqn.state_dict(),\n",
    "                       f\"runs/{'DDQN'}{args.run_name}/{'DDQN'}{args.run_name}_{'episode#'}_{epi}.pt\")\n",
    "            \n",
    "    def log_gradients_in_model(model, logger, step):\n",
    "        for tag, value in model.named_parameters():\n",
    "            if value.grad is not None:\n",
    "                self.logger.add_histogram(tag + \"/grad\", value.grad.cpu(), step)\n",
    "                \n",
    "    def test_ddqn(self,epi):\n",
    "        print('###############Running Test#############################')\n",
    "        train_steps_counter = 0\n",
    "        env = gym.make('Vehicle-fixedSOC')\n",
    "        # env = gym.make('Vehicle-v0')\n",
    "        obs = env.reset()\n",
    "        reward_total_ddqn = []\n",
    "        reward_app_ddqn = []\n",
    "        fuel_total_ddqn = []\n",
    "        fuel_app_ddqn = []\n",
    "        a1 = []\n",
    "        s1 = []\n",
    "        s2 = []\n",
    "        s3 = []\n",
    "        train_steps_counter_all = []\n",
    "        if (epi % args.TEST_MODEL_FREQ ==0):\n",
    "            \n",
    "            for i in range(args.MAX_STEPS):\n",
    "                torch.manual_seed(args.seed)\n",
    "                model = DQN(args.HIDDEN_LAYER_SIZE).to(device)\n",
    "                model.load_state_dict(self.dqn.state_dict())\n",
    "                q_values = model(torch.Tensor(obs).to(device))\n",
    "                if(epi < 2):\n",
    "                    actions = 1000\n",
    "                else:\n",
    "                    actions = torch.argmax(q_values).cpu().numpy()\n",
    "                    # print('q_values:', np.max(q_values.cpu().detach().numpy()))\n",
    "                    q_max = np.max(q_values.cpu().detach().numpy())\n",
    "                    writer.add_scalar(\"test/q_max\", q_max,\n",
    "                                      train_steps_counter)\n",
    "                # print(actions)\n",
    "                next_obs, rewards, dones, infos = env.step(actions)\n",
    "                obs = next_obs\n",
    "                [state1,state2,state3] = obs\n",
    "                fuel_kg = infos[48]\n",
    "                writer.add_scalar(\"test/state1\", state1, train_steps_counter)\n",
    "                writer.add_scalar(\"test/state2\", state2, train_steps_counter)\n",
    "                writer.add_scalar(\"test/state3\", state3, train_steps_counter)\n",
    "                writer.add_scalar(\"test/reward\", rewards, train_steps_counter)\n",
    "                writer.add_scalar(\"test/action\", actions, train_steps_counter)\n",
    "                \n",
    "                train_steps_counter += 1\n",
    "                reward_app_ddqn.append(float(rewards))\n",
    "                fuel_app_ddqn.append(float(fuel_kg))\n",
    "                # a1.append(float(actions))\n",
    "                # s1.append(float(state1))\n",
    "                # s2.append(float(state2))\n",
    "                # s3.append(float(state3))\n",
    "                train_steps_counter_all.append(train_steps_counter)\n",
    "                if dones:\n",
    "                    reward_total_ddqn = sum(reward_app_ddqn)\n",
    "                    fuel_total_ddqn = sum(fuel_app_ddqn)/len(fuel_app_ddqn)\n",
    "                    writer.add_scalar(\"test/reward_total\", reward_total_ddqn, epi)\n",
    "                    writer.add_scalar(\"test/fuel_total_ddqn\",\n",
    "                                      fuel_total_ddqn, epi)\n",
    "                    # self.best_reward = max(self.best_reward , reward_total)\n",
    "                    if(epi < 1):\n",
    "                        self.best_reward = reward_total_ddqn\n",
    "                        self.prev_best_episode = 0\n",
    "                        \n",
    "                    if(self.best_reward > reward_total_ddqn):\n",
    "                        self.best_reward = self.best_reward\n",
    "                        self.prev_best_episode = self.prev_best_episode\n",
    "                        print('###Best reward not changed ,best reward , prev episode###',self.best_reward,self.prev_best_episode)\n",
    "                    else:\n",
    "                        self.best_reward = reward_total_ddqn\n",
    "                        self.prev_best_episode = epi\n",
    "                        print('###Best reward new episode and best new episode is### ',\n",
    "                              self.best_reward, self.prev_best_episode)\n",
    "                        torch.save(self.dqn.state_dict(),\n",
    "                                   f\"runs/{'DDQN'}{args.run_name}/{'DDQN'}{args.run_name}_{'best_reward episode#'}_{epi}.pt\")\n",
    "                        # writer.add_scalar(\"best/s1\", s1, train_steps_counter_all)\n",
    "                        # writer.add_scalar(\"best/s2\", s2, train_steps_counter_all)\n",
    "                        # writer.add_scalar(\"best/s3\", s3, train_steps_counter_all)\n",
    "                        # writer.add_scalar(\"best/a1\", a1, train_steps_counter_all)\n",
    "                        \n",
    "                        \n",
    "            \n",
    "            \n",
    "        \n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeltest = DQN_Agent()\n",
    "# modeltest.dqn.named_parameters()\n",
    "# for name, param in modeltest.dqn.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLTj1P7tnNso"
   },
   "outputs": [],
   "source": [
    "# Instantiating the DQN Agent\n",
    "torch.manual_seed(args.seed)\n",
    "dqn_agent = DQN_Agent()\n",
    "# torch.manual_seed(args.seed)\n",
    "model_dqn = dqn_agent.dqn\n",
    "# writer = SummaryWriter(f\"runs/{'DDQN'}{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UCB_NN(nn.Module):\n",
    "#     def __init__(self, hidden_layer_size):\n",
    "#         super().__init__()\n",
    "#         torch.manual_seed(args.seed)\n",
    "#         self.hidden_layer_size = hidden_layer_size\n",
    "#         # self.fc0 = nn.Linear(number_of_states,self.hidden_layer_size)\n",
    "#         self.fc1 = nn.Linear(1, self.hidden_layer_size)\n",
    "#         # self.fc2 = nn.Linear( self.hidden_layer_size, self.hidden_layer_size)\n",
    "#         # self.fc3 = nn.Linear( self.hidden_layer_size, self.hidden_layer_size)\n",
    "#         # self.fc4 = nn.Linear( self.hidden_layer_size, self.hidden_layer_size)\n",
    "#         self.fc2 = nn.Linear(self.hidden_layer_size, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x = self.fc0(x)\n",
    "#         output = torch.relu(self.fc1(x))\n",
    "#         # output = torch.relu(self.fc2(output))\n",
    "#         # output = torch.relu(self.fc3(output))\n",
    "#         # output = torch.relu(self.fc4(output))\n",
    "#         output = self.fc2(output)\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define UCB class\n",
    "# # Building the brain of the network i.e. the DQN Agent\n",
    "# class UCB(object):\n",
    "#     def __init__(self):\n",
    "#         torch.manual_seed(args.seed)\n",
    "#         self.ucb = UCB_NN(args.UCB_HIDDEN_LAYER_SIZE).to(device)\n",
    "\n",
    "#         # self.criterion = torch.nn.MSELoss()\n",
    "#         self.criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "#         self.optimizer = optim.Adam(params=self.ucb.parameters(), lr=args.UCB_LEARNING_RATE)\n",
    "\n",
    "#     def UCB_calc(self,actions, Q_learned, states, epi):\n",
    "#         self.optimizer = optim.Adam(params=self.ucb.parameters(), lr=args.UCB_LEARNING_RATE)\n",
    "\n",
    "#         actions = torch.Tensor(actions).to(device)\n",
    "#         Q_learned = torch.Tensor(Q_learned).to(device)\n",
    "#         Q_current = self.ucb(actions)\n",
    "#         Q_current = torch.Tensor(Q_current).to(device)\n",
    "#         loss_ucb = (1-args.UCB_FILTER) * (Q_learned - Q_current)\n",
    "#         loss_ucb = self.criterion(Q_learned, Q_current)\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss_ucb.backward()\n",
    "\n",
    "#         # for tag, value in self.ucb.named_parameters():\n",
    "#         #     writer.add_scalar(\"ucb/charts/gradsum \",\n",
    "#         #                       np.array(value.grad.cpu()).sum(), epi)\n",
    "#         #     writer.add_scalar(\"ucb/charts/gradmean\",\n",
    "#         #                       np.array(value.grad.cpu()).mean(), epi)\n",
    "#         #     writer.add_scalar(\"ucb/charts/gradmin\",\n",
    "#         #                       np.array(value.grad.cpu()).min(), epi)\n",
    "#         #     writer.add_scalar(\"ucb/charts/gradmax\",\n",
    "#         #                       np.array(value.grad.cpu()).max(), epi)\n",
    "\n",
    "#         return(Q_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cd66ea83b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the UCB class Agent\n",
    "torch.manual_seed(args.seed)\n",
    "# ucb_agent = UCB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucb_agent.UCB_calc([1], 1, 0.0, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the hyperparameters to tensorboard\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\n",
    "        \"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tag, value in model_dqn.named_parameters():\n",
    "#     print(value.grad.cpu())\n",
    "# # print(writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_counter = 1\n",
    "# for episode in range(NUMBER_OF_EPISODES+1):\n",
    "#     EGREEDY = linear_schedule(0.9, 0.02, NUMBER_OF_EPISODES, e_counter)\n",
    "#     e_counter = e_counter +1\n",
    "#     print(e_counter)\n",
    "#     print(EGREEDY)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 132682,
     "status": "ok",
     "timestamp": 1586934401261,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "QjSSkmLpnNs0",
    "outputId": "51bec1ef-f48a-49a2-ed67-53849e88fa42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.39805\n",
      "Episode: 1\n",
      "self.steps,self.mpge,self.soc 1370 34.56826405002161 -220.5139117683313 0.16041012910096317\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.39805\n",
      "Episode: 1\n",
      "self.steps,self.mpge,self.soc 1370 41.0057614410885 -163.7242435839637 0.6700913480134553\n",
      "loss 0.06944615356596914\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 27.40748792156098 -312.903232686338 0.6717904364759404\n",
      "###Best reward new episode and best new episode is###  -312.903232686338 1\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.3961\n",
      "Episode: 2\n",
      "self.steps,self.mpge,self.soc 1370 44.36086436115212 -142.360909013474 0.24491083756842028\n",
      "loss 0.05949144079530213\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 40.77329118632619 -160.04778759695282 0.4509411872067894\n",
      "###Best reward new episode and best new episode is###  -160.04778759695282 2\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.39415\n",
      "Episode: 3\n",
      "self.steps,self.mpge,self.soc 1370 42.8345926540439 -150.84292303731263 0.6524747729578766\n",
      "loss 0.054445283534152684\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 55.09327351755711 -86.54486144472608 0.3656466648567317\n",
      "###Best reward new episode and best new episode is###  -86.54486144472608 3\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.7\n",
      "Epsilon: 0.39220000000000005\n",
      "Episode: 4\n",
      "self.steps,self.mpge,self.soc 1370 44.58002196257252 -139.65205071458857 0.5378828980498329\n",
      "loss 0.051915786886918364\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 58.012287897021444 -76.85535915672276 0.35647921308974206\n",
      "###Best reward new episode and best new episode is###  -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.39025000000000004\n",
      "Episode: 5\n",
      "self.steps,self.mpge,self.soc 1370 33.753744786730884 -228.18599858977288 0.16097181817985673\n",
      "loss 0.05174073294278607\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 39.12098617242703 -176.99928301441682 0.4847263305426298\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.38830000000000003\n",
      "Episode: 6\n",
      "self.steps,self.mpge,self.soc 1370 43.18788961102931 -149.2751981228015 0.35261785530123463\n",
      "loss 0.052803499095317855\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 35.546741585184044 -205.47289498274347 0.5144787582053463\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.38635\n",
      "Episode: 7\n",
      "self.steps,self.mpge,self.soc 1370 48.24634751245094 -120.13845008858914 0.6152152841595186\n",
      "loss 0.055214557538399316\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 44.84364493203167 -137.445743482713 0.43320028048846587\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.3844\n",
      "Episode: 8\n",
      "self.steps,self.mpge,self.soc 1370 34.3258696832239 -223.58483946310832 0.16382257629247057\n",
      "loss 0.05796534354459029\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 49.351480970628 -113.18233150566063 0.4025206400008327\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.38245\n",
      "Episode: 9\n",
      "self.steps,self.mpge,self.soc 1370 34.19092827255498 -224.51994890994487 0.16418286905119597\n",
      "loss 0.06158148903644914\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 38.34080913355379 -183.41134998004094 0.49255837812441505\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.3805\n",
      "Episode: 10\n",
      "self.steps,self.mpge,self.soc 1370 42.29159646248954 -154.90359559043713 0.35874071706156213\n",
      "loss 0.0663978460449603\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 56.07462147778658 -84.91131852227564 0.3692337611915324\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.37855\n",
      "Episode: 11\n",
      "self.steps,self.mpge,self.soc 1370 33.30515621371709 -233.75820027809564 0.17108390363247683\n",
      "loss 0.07233972476021142\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 52.72614161764986 -97.88898595542415 0.3841329359854116\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.37660000000000005\n",
      "Episode: 12\n",
      "self.steps,self.mpge,self.soc 1370 46.803513151629446 -128.51548990232556 0.3276954004560884\n",
      "loss 0.078935562143393\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 49.75829736866914 -109.49463157751183 0.3939095973318155\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.37465000000000004\n",
      "Episode: 13\n",
      "self.steps,self.mpge,self.soc 1370 49.05553392611057 -114.89042957291174 0.405828749267393\n",
      "loss 0.08838128170458301\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 55.86374652471316 -83.41053254561672 0.361280575276135\n",
      "###Best reward not changed ,best reward , prev episode### -76.85535915672276 4\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.37270000000000003\n",
      "Episode: 14\n",
      "self.steps,self.mpge,self.soc 1370 51.13898009803338 -105.21719601853995 0.2941894118078392\n",
      "loss 0.10306565549920847\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 61.06772374885083 -65.84898754324408 0.34086050265243634\n",
      "###Best reward new episode and best new episode is###  -65.84898754324408 14\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.7\n",
      "Epsilon: 0.37075\n",
      "Episode: 15\n",
      "self.steps,self.mpge,self.soc 1370 55.66875342692872 -86.52191661238527 0.47137449932442904\n",
      "loss 0.12909526549640699\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 68.62580473354474 -44.418482637372314 0.31394060374762184\n",
      "###Best reward new episode and best new episode is###  -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.3688\n",
      "Episode: 16\n",
      "self.steps,self.mpge,self.soc 1370 34.26428561135612 -222.6947671460445 0.1601347548704434\n",
      "loss 0.17108788474757494\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 67.85947561439758 -46.68280284953077 0.31753049425357205\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.36685\n",
      "Episode: 17\n",
      "self.steps,self.mpge,self.soc 1370 55.950417341064195 -86.14740697671948 0.5726767830218134\n",
      "loss 0.23522939993642344\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 66.57349638328382 -50.4674953569715 0.3234136015191102\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.3649\n",
      "Episode: 18\n",
      "self.steps,self.mpge,self.soc 1370 53.525993998979516 -96.09268624645986 0.5860241101407156\n",
      "loss 0.32142613585211416\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.51754530295171 -62.37643824485528 0.33920774977212836\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.36295\n",
      "Episode: 19\n",
      "self.steps,self.mpge,self.soc 1370 54.07292703456049 -93.46976303931513 0.1817133748871077\n",
      "loss 0.43209426959471775\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 58.68238594291807 -74.7142133195599 0.35423383379636053\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.7\n",
      "Epsilon: 0.36100000000000004\n",
      "Episode: 20\n",
      "self.steps,self.mpge,self.soc 1370 52.32020130924878 -100.72552839349775 0.4905184038438274\n",
      "loss 0.553367220404115\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 55.120055912699556 -88.19955898228953 0.3721523324869932\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.35905000000000004\n",
      "Episode: 21\n",
      "self.steps,self.mpge,self.soc 1370 34.53838219188783 -220.5027218612076 0.16041285027856078\n",
      "loss 0.6875082819771736\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 59.1887964421336 -72.87754458061877 0.3516636203047756\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.35710000000000003\n",
      "Episode: 22\n",
      "self.steps,self.mpge,self.soc 1370 34.33837777130397 -222.1679254324982 0.16042033249988383\n",
      "loss 0.8312072647651738\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 60.9638293683729 -67.27431661288146 0.34548415361942497\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.35515\n",
      "Episode: 23\n",
      "self.steps,self.mpge,self.soc 1370 42.27076042254241 -155.24977176778413 0.16042033319494423\n",
      "loss 0.9875212484385577\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.31469054067065 -62.949315792030646 0.3397647558098986\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.3532\n",
      "Episode: 24\n",
      "self.steps,self.mpge,self.soc 1370 52.950551846393374 -98.22105652358573 0.38773275287045217\n",
      "loss 1.1451719624425971\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 61.307929770541534 -65.856093748372 0.34263258194656726\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.35125\n",
      "Episode: 25\n",
      "self.steps,self.mpge,self.soc 1370 52.72132730976448 -99.432518957223 0.2899138143042384\n",
      "loss 1.3054551456216654\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.39112755125493 -62.72130532384565 0.33952535327225797\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.3493\n",
      "Episode: 26\n",
      "self.steps,self.mpge,self.soc 1370 34.28841012043072 -222.82739802890933 0.16114405382799152\n",
      "loss 1.4657775992411954\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 64.56442708108604 -56.05525290429857 0.33050433973701665\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.34735000000000005\n",
      "Episode: 27\n",
      "self.steps,self.mpge,self.soc 1370 54.488364622524195 -91.80567037032984 0.5797085458957258\n",
      "loss 1.6274733041812663\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 63.745447807868636 -58.507683209363414 0.33380908729882947\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.34540000000000004\n",
      "Episode: 28\n",
      "self.steps,self.mpge,self.soc 1370 34.497097838668665 -220.95740405890749 0.16041233337435207\n",
      "loss 1.7910841814983143\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.51427878457429 -62.275650261100004 0.3387866082267843\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.34345000000000003\n",
      "Episode: 29\n",
      "self.steps,self.mpge,self.soc 1370 55.229878788966516 -88.89954455062448 0.17608398805295775\n",
      "loss 1.9529423760150513\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 64.30104005818842 -56.790029318550665 0.33137877533434384\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.3415\n",
      "Episode: 30\n",
      "self.steps,self.mpge,self.soc 1370 56.970848920345304 -81.90437212639442 0.2660887330644757\n",
      "loss 2.1020538797554438\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 61.99290113960251 -64.26195402776395 0.34227228091220485\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.33955\n",
      "Episode: 31\n",
      "self.steps,self.mpge,self.soc 1370 57.4516932425642 -80.29470916391344 0.5645804172022425\n",
      "loss 2.243574851845287\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 64.93148041771255 -55.06437502182775 0.3293888119783172\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.3376\n",
      "Episode: 32\n",
      "self.steps,self.mpge,self.soc 1370 34.23123518866693 -223.01562059883824 0.16041480885027554\n",
      "loss 2.377983636009848\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 66.88089404069162 -49.664817644887584 0.3224467383412475\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.33565\n",
      "Episode: 33\n",
      "self.steps,self.mpge,self.soc 1370 56.03966865538393 -85.85625950884513 0.3724252827977565\n",
      "loss 2.5034598592365898\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 63.861727689443306 -58.489353357851826 0.33463095690871264\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.3337\n",
      "Episode: 34\n",
      "self.steps,self.mpge,self.soc 1370 55.92970884544358 -85.92511943195986 0.17163617430196945\n",
      "loss 2.622520709591748\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 57.0033197797772 -80.7904399252119 0.36186384347687456\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.33175\n",
      "Episode: 35\n",
      "self.steps,self.mpge,self.soc 1370 34.59720062985038 -220.50144807296314 0.16041925767457407\n",
      "loss 2.735635329893831\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 66.89546922355507 -49.5756539162063 0.32220356476912915\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.32980000000000004\n",
      "Episode: 36\n",
      "self.steps,self.mpge,self.soc 1370 34.353771868163214 -222.20501277766124 0.16042033319446178\n",
      "loss 2.8440378244233666\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.177432137614105 -63.514124475183706 0.3406097766889678\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.32785000000000003\n",
      "Episode: 37\n",
      "self.steps,self.mpge,self.soc 1370 53.87998137662448 -94.1612919968072 0.2820877296911033\n",
      "loss 2.9437902795113624\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 55.70924312311838 -85.74770051211256 0.36835971689290037\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.3259\n",
      "Episode: 38\n",
      "self.steps,self.mpge,self.soc 1370 34.29358932303218 -222.7418905853635 0.16042022891366992\n",
      "loss 3.0363561582500687\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 50.778568952324 -107.09352637025607 0.3966987812267221\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.32395\n",
      "Episode: 39\n",
      "self.steps,self.mpge,self.soc 1370 34.09128110995839 -224.45499814858792 0.16042033326844413\n",
      "loss 3.1236883396925164\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 55.577939728007 -86.60746071620278 0.370847266102215\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.322\n",
      "Episode: 40\n",
      "self.steps,self.mpge,self.soc 1370 50.487689886520734 -108.88248957240948 0.20045204769015024\n",
      "loss 3.2032502858927425\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 48.73750547010887 -116.0492226250414 0.4056992265150262\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.32005\n",
      "Episode: 41\n",
      "self.steps,self.mpge,self.soc 1370 52.83407078804198 -98.89013386537131 0.3891258270034569\n",
      "loss 3.2769972465409505\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 51.58716633690001 -102.99114386373415 0.39068717764436733\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.31810000000000005\n",
      "Episode: 42\n",
      "self.steps,self.mpge,self.soc 1370 42.09820597190874 -156.31895871758155 0.16168880200289587\n",
      "loss 3.345886878412435\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 58.23666648510808 -77.22044868406745 0.35982135524614456\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.31615000000000004\n",
      "Episode: 43\n",
      "self.steps,self.mpge,self.soc 1370 42.32460581034024 -155.09806634364597 0.16081425341301556\n",
      "loss 3.4110751571783178\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 57.88045253958895 -77.7734752572376 0.35894978888446505\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.31420000000000003\n",
      "Episode: 44\n",
      "self.steps,self.mpge,self.soc 1370 55.41880531482734 -88.35537039361468 0.5757929052682138\n",
      "loss 3.473261031869818\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 44.55594384670502 -138.95162423687813 0.4343304041147504\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.31225\n",
      "Episode: 45\n",
      "self.steps,self.mpge,self.soc 1370 49.90252906763984 -111.85619471732004 0.30478271706050664\n",
      "loss 3.5323206609191953\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 50.83641975505646 -106.84877288333031 0.3968491670065941\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.3103\n",
      "Episode: 46\n",
      "self.steps,self.mpge,self.soc 1370 56.39895641618985 -84.34656630227289 0.570136907076076\n",
      "loss 3.585357245410746\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 59.249215662301566 -72.68157233399548 0.35167009790220466\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.30835\n",
      "Episode: 47\n",
      "self.steps,self.mpge,self.soc 1370 33.50113911922054 -230.39384423834855 0.16882259010439274\n",
      "loss 3.6357328244072513\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 58.95624190979488 -73.58259517847468 0.35257285441090314\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.3064\n",
      "Episode: 48\n",
      "self.steps,self.mpge,self.soc 1370 53.25919640579257 -96.67365699259963 0.38532940242015684\n",
      "loss 3.6818741209485726\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 54.46265900960342 -91.52063242884658 0.3780419695356751\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.30445\n",
      "Episode: 49\n",
      "self.steps,self.mpge,self.soc 1370 54.190738809546254 -93.10246889668885 0.5814144362076108\n",
      "loss 3.7238681726698015\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 49.64511192755029 -112.78223452264301 0.40516290775770913\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.3025\n",
      "Episode: 50\n",
      "self.steps,self.mpge,self.soc 1370 46.67907514951611 -128.35979024318493 0.3256072777563664\n",
      "loss 3.7628883456443676\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 60.05113770040241 -70.58974843299464 0.3502769844346265\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.30055\n",
      "Episode: 51\n",
      "self.steps,self.mpge,self.soc 1370 46.380028445916146 -129.91194462860642 0.22707555456361617\n",
      "loss 3.798948898209946\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 53.34901771275219 -95.48502235639414 0.38148512907859816\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.29860000000000003\n",
      "Episode: 52\n",
      "self.steps,self.mpge,self.soc 1370 42.7378074654708 -153.1993635890616 0.1604202839494484\n",
      "loss 3.831388922572645\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 57.919167115595506 -78.14103206925152 0.3607086774927782\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.29665\n",
      "Episode: 53\n",
      "self.steps,self.mpge,self.soc 1370 34.49607115975022 -221.51768901913638 0.16042033265420386\n",
      "loss 3.859891933492811\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 57.69734690488748 -78.2812225204654 0.35881202009108976\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.2947\n",
      "Episode: 54\n",
      "self.steps,self.mpge,self.soc 1370 55.89168858416403 -86.44351610497456 0.37323069605662085\n",
      "loss 3.8861826728773727\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 60.79791819451921 -68.17263180059396 0.3475628003978209\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.29275\n",
      "Episode: 55\n",
      "self.steps,self.mpge,self.soc 1370 42.644275547514134 -153.57081282301178 0.16112015109641908\n",
      "loss 3.908833150066808\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 51.8430674782617 -102.70870715721291 0.3928854430253451\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.2908\n",
      "Episode: 56\n",
      "self.steps,self.mpge,self.soc 1370 54.05660328325495 -93.83551108450229 0.5828473321092954\n",
      "loss 3.9301544864900384\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 52.15818516173211 -100.9998850731305 0.3896905269263187\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.28885\n",
      "Episode: 57\n",
      "self.steps,self.mpge,self.soc 1370 50.660978986716216 -108.59457766455976 0.40140942145807984\n",
      "loss 3.947747473356479\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 50.754975417353265 -106.85014397874917 0.3955418872281211\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.28690000000000004\n",
      "Episode: 58\n",
      "self.steps,self.mpge,self.soc 1370 42.15513501355954 -155.96057739284555 0.160420333206431\n",
      "loss 3.963252550155455\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 53.60520045020239 -95.21324784199939 0.38355905576583676\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.3\n",
      "Epsilon: 0.28495000000000004\n",
      "Episode: 59\n",
      "self.steps,self.mpge,self.soc 1370 42.47967693196052 -154.34550701534758 0.16041924676978023\n",
      "loss 3.9774853513429758\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 57.768318936984144 -78.38270637945382 0.35990019091745884\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.28300000000000003\n",
      "Episode: 60\n",
      "self.steps,self.mpge,self.soc 1370 57.82246034465119 -79.06549264070475 0.5632510236408225\n",
      "loss 3.992027477805766\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.060982834569764 -63.85669328769498 0.3412708324632045\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.7\n",
      "Epsilon: 0.28105\n",
      "Episode: 61\n",
      "self.steps,self.mpge,self.soc 1370 54.18357223921941 -93.10934359564432 0.48144078178618543\n",
      "loss 4.007520995281941\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 63.522373598949024 -59.24826239508435 0.3349622638916663\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.2791\n",
      "Episode: 62\n",
      "self.steps,self.mpge,self.soc 1370 34.26729887348464 -222.5060649367021 0.16041883847814584\n",
      "loss 4.02546966683966\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 63.43977061856817 -59.62852696515378 0.3357916935288605\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.27715\n",
      "Episode: 63\n",
      "self.steps,self.mpge,self.soc 1370 56.01801400276346 -85.88286562338294 0.37242518073165043\n",
      "loss 4.036599759940295\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 61.1584884802141 -66.16098257099596 0.3425300979459486\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.7\n",
      "Epsilon: 0.2752\n",
      "Episode: 64\n",
      "self.steps,self.mpge,self.soc 1370 51.22584942567944 -106.10986346925421 0.4985429099252548\n",
      "loss 4.045467651155257\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 61.80922952218001 -64.84943073449102 0.3430875372342805\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.27325\n",
      "Episode: 65\n",
      "self.steps,self.mpge,self.soc 1370 55.78878203553435 -86.72708951289331 0.3733565980235189\n",
      "loss 4.052777724816785\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 62.27256174494628 -63.29469471719622 0.3407922216562566\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.6\n",
      "Epsilon: 0.2713\n",
      "Episode: 66\n",
      "self.steps,self.mpge,self.soc 1370 50.318127308702685 -110.28521399897699 0.4036469428610709\n",
      "loss 4.058425833487344\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 60.10351689426609 -70.49733392165963 0.3505659311255487\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.26935\n",
      "Episode: 67\n",
      "self.steps,self.mpge,self.soc 1370 34.11644254451865 -225.0112845816952 0.16307650308212\n",
      "loss 4.062984379833152\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 54.21594227612826 -92.2868315912667 0.37847913572676406\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.2674\n",
      "Episode: 68\n",
      "self.steps,self.mpge,self.soc 1370 34.32470830404101 -222.4641530354965 0.16042033327238986\n",
      "loss 4.0661491343052605\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 44.54879519440498 -139.48876614529848 0.43623282851902767\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.26545\n",
      "Episode: 69\n",
      "self.steps,self.mpge,self.soc 1370 53.70303171524647 -95.02005505191623 0.5834936229362023\n",
      "loss 4.067732481409862\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 44.131742273967276 -141.31761930053634 0.4366700657732536\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.2635\n",
      "Episode: 70\n",
      "self.steps,self.mpge,self.soc 1370 48.23517607189522 -120.50009648551585 0.21643439172201298\n",
      "loss 4.067476531388328\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 54.049328193194235 -92.76671193014089 0.3784932009114924\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.5\n",
      "Epsilon: 0.26155\n",
      "Episode: 71\n",
      "self.steps,self.mpge,self.soc 1370 46.63997698993497 -128.87972689940142 0.3267670952760715\n",
      "loss 4.067002752185755\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 47.09017038342537 -125.82048449089893 0.421285775763215\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.2\n",
      "Epsilon: 0.25960000000000005\n",
      "Episode: 72\n",
      "self.steps,self.mpge,self.soc 1370 34.62603036525865 -220.16854076201946 0.16042033312183657\n",
      "loss 4.0647604616763875\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 53.29892011274436 -96.35267431350715 0.38417229715265705\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.8\n",
      "Epsilon: 0.25765000000000005\n",
      "Episode: 73\n",
      "self.steps,self.mpge,self.soc 1370 50.957470672571176 -107.61679114084532 0.6011651086986507\n",
      "loss 4.062320290733973\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n",
      "self.steps,self.mpge,self.soc 1370 49.84935892556279 -111.85216427117298 0.4038405511564373\n",
      "###Best reward not changed ,best reward , prev episode### -44.418482637372314 15\n",
      "###############Running Trainning#############################\n",
      "initial soc is 0.4\n",
      "Epsilon: 0.25570000000000004\n",
      "Episode: 74\n",
      "self.steps,self.mpge,self.soc 1370 49.54149996172641 -113.65235266838786 0.20716451607024836\n",
      "loss 4.060456564352265\n",
      "###############Running Test#############################\n",
      "initial soc is 0.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "steps_total = []\n",
    "reward_total_all_episodes = []\n",
    "steps_counter = 0\n",
    "episode_counter = []\n",
    "e_counter = 1\n",
    "episode_done = False\n",
    "loss_step_total = []\n",
    "q_pred_step_total=[]\n",
    "q_targ_step_total=[]\n",
    "for episode in range(args.NUMBER_OF_EPISODES+1):\n",
    "    print('###############Running Trainning#############################')\n",
    "    env = gym.make('Vehicle-v0') \n",
    "    \n",
    "    episode_counter.append(episode)\n",
    "    if episode == args.NUMBER_OF_EPISODES:\n",
    "        episode_done = True\n",
    "        \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    # print('episode :',e_counter)\n",
    "    LR_comp = linear_schedule(\n",
    "        args.LEARNING_RATE, args.LEARNING_RATE/20, args.NUMBER_OF_EPISODES, e_counter)\n",
    "    writer.add_scalar(\"train/learning_rate\", LR_comp, episode)\n",
    "\n",
    "    step = 0\n",
    "    reward_total = []\n",
    "    reward_app = []\n",
    "    # EGREEDY = calculate_epsilon(NUMBER_OF_EPISODES)\n",
    "    if episode<args.NUMBER_OF_EPISODES-10:\n",
    "        EGREEDY = linear_schedule(args.EGREEDY_START, args.EGREEDY_FINAL, args.NUMBER_OF_EPISODES, e_counter)\n",
    "        # LR_comp = linear_schedule(\n",
    "        #     args.LEARNING_RATE, 0.0001, args.NUMBER_OF_EPISODES, e_counter)\n",
    "    else:\n",
    "        EGREEDY = linear_schedule(\n",
    "            args.EGREEDY_START, args.EGREEDY_FINAL, args.NUMBER_OF_EPISODES, e_counter)\n",
    "        # LR_comp = linear_schedule(\n",
    "        #     args.LEARNING_RATE, 0.0001, args.NUMBER_OF_EPISODES, e_counter)\n",
    "    print('Epsilon:', EGREEDY)\n",
    "    print('Episode:',e_counter)\n",
    "   \n",
    "    for i in range(args.MAX_STEPS):\n",
    "        step += 1\n",
    "        steps_counter += 1\n",
    "        \n",
    "        [state1,state2,state3] = state\n",
    "        writer.add_scalar(\"train/states/state1\", state1, steps_counter)\n",
    "        writer.add_scalar(\"train/states/state2\", state2, steps_counter)\n",
    "        writer.add_scalar(\"train/states/state3\", state3, steps_counter)\n",
    "        \n",
    "        writer.add_scalar(\"train/state1\", state1, step)\n",
    "        writer.add_scalar(\"train/state2\", state2, step)\n",
    "        writer.add_scalar(\"train/state3\", state3, step)\n",
    "        \n",
    "        \n",
    "        # if(state3 > -0.6):\n",
    "        #     action = 1999\n",
    "        # else:\n",
    "        #     action = dqn_agent.select_action(state, EGREEDY)\n",
    "        \n",
    "        # if(state3 > -0.6):\n",
    "        #     action = 1999\n",
    "        # else:\n",
    "        #     action = dqn_agent.select_action(state, EGREEDY)\n",
    "            \n",
    "        action = dqn_agent.select_action(state, EGREEDY)\n",
    "            \n",
    "        writer.add_scalar(\"train/action/action\", action, steps_counter)\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        reward_app.append(reward)\n",
    "        if(episode > args.TRAINING_STARTS):\n",
    "            loss_step = dqn_agent.optimize(steps_counter,LR_comp)\n",
    "            if loss_step == None:\n",
    "                loss_step = 0\n",
    "                q_pred_step = 0\n",
    "                q_targ_step = 0\n",
    "            loss_step_total.append(loss_step)\n",
    "            # q_pred_step_total.append(q_pred_step)\n",
    "            # q_targ_step_total.append(q_targ_step)\n",
    "            state = new_state\n",
    "            # os.system('cls')\n",
    "            # UU = ucb_agent.UCB_calc(1, 1.0, state, episode)\n",
    "            writer.add_scalar(\"train/charts/epsilon\", EGREEDY, episode)\n",
    "            if done:\n",
    "                steps_total.append(step)\n",
    "                reward_total = sum(reward_app)\n",
    "                writer.add_scalar(\"train/reward\", reward_total, episode)\n",
    "                reward_total_all_episodes.append(reward_total)\n",
    "                print('loss',sum(loss_step_total)/len(loss_step_total))\n",
    "                writer.add_scalar(\"train/losses/td_loss\",sum(loss_step_total)/len(loss_step_total), episode)\n",
    "                # writer.add_scalar(\"losses/predicted\", sum(q_pred_step_total)/len(q_pred_step_total), episode)\n",
    "                # writer.add_scalar(\"losses/target\", sum(q_targ_step_total)/len(q_targ_step_total), episode)\n",
    "                # for tag, value in model_dqn.named_parameters():\n",
    "                #     writer.add_scalar(\"charts/gradsum \", np.array(value.grad.cpu()).sum(), episode)\n",
    "                #     writer.add_scalar(\"charts/gradmean\", np.array(value.grad.cpu()).mean(), episode)\n",
    "                #     writer.add_scalar(\"charts/gradmin\", np.array(value.grad.cpu()).min(), episode)\n",
    "                #     writer.add_scalar(\"charts/gradmax\", np.array(value.grad.cpu()).max(), episode) \n",
    "                e_counter = e_counter+1\n",
    "                dqn_agent.save_model(episode, args.NUMBER_OF_EPISODES)\n",
    "                # writer.add_scalar(\"charts/episodic_return\",\n",
    "                #                       info[\"episode\"][\"r\"], episode)\n",
    "                #     # writer.add_scalar(\"charts/episodic_length\",\n",
    "                                    #   info[\"episode\"][\"l\"], global_step)\n",
    "                dqn_agent.test_ddqn(episode)\n",
    "                break\n",
    "            # if episode_done:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average reward: %.2f\" % (sum(reward_total_all_episodes)/NUMBER_OF_EPISODES))\n",
    "# print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Average reward: %.2f\" % (sum(steps_total)/NUMBER_OF_EPISODES))\n",
    "# print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_filter(panda_series, window_size):\n",
    "    numbers_series = pd.Series(panda_series)\n",
    "    windows = numbers_series.rolling(window_size)\n",
    "    moving_averages = windows.mean()\n",
    "    moving_averages_list = moving_averages.tolist()\n",
    "    # mvoing_average_series = list(moving_averages_list[window_size - 1:])\n",
    "    # mvoing_average_series[:1369] = [0]\n",
    "    # mvoing_average_series[:1370] = [0]\n",
    "    return(moving_averages_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "df_ck = pd.DataFrame()\n",
    "df_ck['x'] = np.array(reward_total_all_episodes)\n",
    "df_ck['moving'] = moving_average_filter(df_ck.x.values, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=episode_counter, y=reward_total_all_episodes,\n",
    "                         mode='lines',\n",
    "                         name='regular'))\n",
    "fig.add_trace(go.Scatter(x=episode_counter, y=df_ck.moving,\n",
    "                         mode='lines',\n",
    "                         name='regular'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 132676,
     "status": "ok",
     "timestamp": 1586934401262,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "O2Wb5zU_nNs6",
    "outputId": "2b6ed7e6-5397-41df-e41c-5119c3cf09f3"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,5))\n",
    "# plt.title(\"Rewards Collected\")\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Reward')\n",
    "# plt.bar(np.arange(len(steps_total)), steps_total, alpha=0.5, color='green', width=6)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UCB (Q):\n",
    "    Q_init = 0\n",
    "    i = 0\n",
    "    Q_new_lis = []\n",
    "    i_lis =[]\n",
    "    for i,q in enumerate(Q):\n",
    "        # print(q)\n",
    "        # print('OK')\n",
    "        Q_current = Q_init\n",
    "        loss = 0.1 * (q - Q_current)\n",
    "        Q_new = Q_current + loss\n",
    "        Q_init = Q_new\n",
    "        i = i+1\n",
    "        \n",
    "      \n",
    "    \n",
    "        Q_new_lis.append(Q_init)\n",
    "        i_lis.append(i)\n",
    "        plt.scatter(i_lis,Q_new_lis)\n",
    "        \n",
    "         \n",
    "    return(Q_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCB(df.A.values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#create DataFrame\n",
    "df = pd.DataFrame(np.random.randint(0, 100, size=(100000, 1)), columns=list('A'))\n",
    "\n",
    "#view DataFrame\n",
    "df.mean()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQN__ExperienceReplay_TargetNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "clean_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd7a6cd272f30e4f6e264e6677d4e63cfd962276a42f91601c3e643110df28fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
