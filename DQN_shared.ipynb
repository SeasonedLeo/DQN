{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CW6TGEyznNqu"
   },
   "outputs": [],
   "source": [
    "# Importing the required library\n",
    "import sys\n",
    "import os \n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time \n",
    "import argparse\n",
    "sys.argv = ['']\n",
    "del sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-v_1\", \"--verbose_1\", help=\" \",\n",
    "                    action=\"store_true\", default='Original+linear layer to see if Q is negative ' )\n",
    "parser.add_argument(\"-seed\", \"--seed\", help=\"increase output verbosity\",\n",
    "                    action=\"store_true\",default = 1)\n",
    "parser.add_argument(\"-NUMBER_OF_EPISODES\", \"--NUMBER_OF_EPISODES\", help=\"Total_Episodes to run \",\n",
    "                    action=\"store_true\",default = 200)\n",
    "parser.add_argument(\"-MAX_STEPS\", \"--MAX_STEPS\", help=\"MAX_STEPS to run each episode \",\n",
    "                    action=\"store_true\",default = 1369)\n",
    "parser.add_argument(\"-LEARNING_RATE\", \"--LEARNING_RATE\", help=\"LEARNING_RATE \",\n",
    "                    action=\"store_true\",default = 0.0001) #0.0001\n",
    "\n",
    "parser.add_argument(\"-UCB_LEARNING_RATE\", \"--UCB_LEARNING_RATE\", help=\"UCB_LEARNING_RATE \",\n",
    "                    action=\"store_true\",default = 0.0001) #0.0001\n",
    "\n",
    "parser.add_argument(\"-UCB_FILTER\", \"--UCB_FILTER\", help=\"UCB_FILTER \",\n",
    "                    action=\"store_true\",default = 0.0001)\n",
    "\n",
    "parser.add_argument(\"-DISCOUNT_FACTOR\", \"--DISCOUNT_FACTOR\", help=\"DISCOUNT_FACTOR \",\n",
    "                    action=\"store_true\",default = 0.99)\n",
    "\n",
    "parser.add_argument(\"-HIDDEN_LAYER_SIZE\", \"--HIDDEN_LAYER_SIZE\", help=\"HIDDEN_LAYER_SIZE \", \n",
    "                    action=\"store_true\",default = 128)\n",
    "\n",
    "parser.add_argument(\"-UCB_HIDDEN_LAYER_SIZE\", \"--UCB_HIDDEN_LAYER_SIZE\", help=\"UCB HIDDEN_LAYER_SIZE \",\n",
    "                    action=\"store_true\", default=128)\n",
    "\n",
    "parser.add_argument(\"-EGREEDY_START\", \"--EGREEDY_START\", help=\"EGREEDY_START \",\n",
    "                    action=\"store_true\",default = 0.4) #0.4\n",
    "parser.add_argument(\"-EGREEDY_FINAL\", \"--EGREEDY_FINAL\", help=\"EGREEDY_FINAL \",\n",
    "                    action=\"store_true\",default = 0.01) #0.01\n",
    "parser.add_argument(\"-EGREEDY_DECAY\", \"--EGREEDY_DECAY\", help=\"EGREEDY_DECAY- not being used \",\n",
    "                    action=\"store_true\",default = 2000)\n",
    "parser.add_argument(\"-REPLAY_BUFFER_SIZE\", \"--REPLAY_BUFFER_SIZE\", help=\"REPLAY_BUFFER_SIZE \",\n",
    "                    action=\"store_true\",default = 40000)\n",
    "parser.add_argument(\"-BATCH_SIZE\", \"--BATCH_SIZE\", help=\"BATCH_SIZE \",\n",
    "                    action=\"store_true\",default = 64)\n",
    "parser.add_argument(\"-UPDATE_TARGET_FREQUENCY\", \"--UPDATE_TARGET_FREQUENCY\", help=\"UPDATE_TARGET_FREQUENCY \",\n",
    "                    action=\"store_true\", default=2000)\n",
    "parser.add_argument(\"-TEST_MODEL_FREQ\", \"--TEST_MODEL_FREQ\", type=int, help=\"After How many episodes you want to test the model \",\n",
    "                     default=1)\n",
    "\n",
    "parser.add_argument(\"-TEST_RUN_EPISODE\", \"-TEST_RUN_EPISODE\", help=\" Number of episodes to run while testing . \",\n",
    "                    action=\"store_true\", default=10)\n",
    "\n",
    "\n",
    "parser.add_argument(\"-MODEL_SAVE_FREQUENCY\", \"--MODEL_SAVE_FREQUENCY\", help=\"After How many episodes you want to save the model \",\n",
    "                    action=\"store_true\", default=10)\n",
    "\n",
    "parser.add_argument(\"-TRAINING_STARTS\", \"--TRAINING_STARTS\", type=int, help=\"After How many episodes does the optimization starts,currently 10 cycles. \",\n",
    "                     default=0)\n",
    "\n",
    "parser.add_argument(\"-TAU\", \"--TAU\", type=int, help=\"Target Network Update.,tau = 0 no update , tau = 1 imediate update of target with latest, new_target = q * tau + (1-tau) * target \",\n",
    "                     default=0.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument(\"-run_name\", \"--run_name\", help=\"run name\",\n",
    "                    action=\"store_true\", default=f\"{'_vehcile_13_random_initial_SOC'}_{int(time.time())}\")\n",
    "args = parser.parse_args()\n",
    "# args = argparse.Namespace(verbose=False, verbose_1=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/{'DDQN'}{args.run_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5052,
     "status": "ok",
     "timestamp": 1586934273584,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "4WvcVL8VnNrQ",
    "outputId": "64eee3ac-d95f-4645-eee5-ffe9d8f13eea"
   },
   "outputs": [],
   "source": [
    "# selecting the available device (cpu/gpu)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZWZft9z6nNrm"
   },
   "outputs": [],
   "source": [
    "# Creating the CartPole-v0 environment\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "# Creating the CartPole-v0 environment\n",
    "\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make('Vehicle-fixedSOC')\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "new_state, reward, done, info = env.step(action)\n",
    "# new_state[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2v8unZtnNrz"
   },
   "outputs": [],
   "source": [
    "# setting the seed value for reproducibility\n",
    "\n",
    "# seed = 1626\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "# run_name = f\"{seed}__{int(time.time())}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6627,
     "status": "ok",
     "timestamp": 1586934275172,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "79XQqTKSnNr8",
    "outputId": "f0ef9d80-1b41-4ba6-ff58-2c3e214d8629"
   },
   "outputs": [],
   "source": [
    "# Fetching the number of states and actions\n",
    "#number_of_states = env.observation_space.n\n",
    "number_of_states = env.observation_space.shape[0]\n",
    "number_of_actions = env.action_space.n\n",
    "# checking the total number of states and action\n",
    "print('Total number of States : {}'.format(number_of_states)) \n",
    "print('Total number of Actions : {}'.format(number_of_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(EGREEDY: float, EGREEDY_FINAL: float, duration: int, t: int):\n",
    "    slope = (EGREEDY_FINAL - EGREEDY) / duration\n",
    "    return max(slope * t + EGREEDY, EGREEDY_FINAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CqEC_YRRnNsS"
   },
   "outputs": [],
   "source": [
    "# Deep Q Network Model Architecture\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self , hidden_layer_size):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(args.seed)\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.fc1 = nn.Linear(number_of_states, self.hidden_layer_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_layer_size,number_of_actions)\n",
    "        \n",
    "    # def _init_weights(self, module):\n",
    "    #     if isinstance(module, nn.Linear):\n",
    "    #         module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "    #         if module.bias is not None:\n",
    "    #             module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.fc0(x)\n",
    "        output = torch.relu(self.fc1(x))\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLrMhP3UnNsW"
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    def __init__(self , capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.pointer = 0\n",
    "    \n",
    "    def push(self , state, action, new_state, reward, done):\n",
    "        experience = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.pointer >= len(self.buffer):\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.pointer] = experience\n",
    "        \n",
    "        self.pointer = (self.pointer + 1) % self.capacity\n",
    "        \n",
    "    def sample(self , batch_size):\n",
    "        # random.seed(args.seed)\n",
    "        return zip(*random.sample(self.buffer , batch_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoqCEYxnnNsc"
   },
   "outputs": [],
   "source": [
    "# Instantiating the ExperienceReplay\n",
    "memory = ExperienceReplay(args.REPLAY_BUFFER_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZO2ic3VVnNsi"
   },
   "outputs": [],
   "source": [
    "# Building the brain of the network i.e. the DQN Agent\n",
    "\n",
    "class DQN_Agent(object):\n",
    "    def __init__(self):\n",
    "        torch.manual_seed(args.seed)\n",
    "        self.dqn = DQN(args.HIDDEN_LAYER_SIZE).to(device)\n",
    "        self.target_dqn = DQN(args.HIDDEN_LAYER_SIZE).to(device)\n",
    "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "        \n",
    "        # self.criterion = torch.nn.MSELoss()\n",
    "        self.criterion = torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=5.0)\n",
    "        \n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.dqn.parameters(), lr=args.LEARNING_RATE)\n",
    "        \n",
    "        self.target_dqn_update_counter = 0\n",
    "        self.best_reward = -99999\n",
    "        self.prev_best_episode = 0\n",
    "    \n",
    "    def select_action(self, state, EGREEDY):\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        if random_for_egreedy > EGREEDY:      \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                state = torch.Tensor(state).to(device)\n",
    "                q_values = self.dqn(state) # + round(random.uniform(0.1, 5), 1)\n",
    "                action = torch.max(q_values,0)[1]\n",
    "                action = action.item()        \n",
    "        else:\n",
    "            # random.seed(args.seed)\n",
    "            action = env.action_space.sample()\n",
    "            # action = 1998\n",
    "            # print(action)\n",
    "            \n",
    "        # print(action)\n",
    "        return action\n",
    "    \n",
    "    def optimize(self,epi,LR):\n",
    "        self.optimizer = optim.Adam(\n",
    "            params=self.dqn.parameters(), lr=LR)\n",
    "        \n",
    "        if (args.BATCH_SIZE > len(memory)):\n",
    "            return\n",
    "        \n",
    "        state, action, new_state, reward, done = memory.sample(args.BATCH_SIZE)\n",
    "        # print(reward)\n",
    "        \n",
    "        state = torch.Tensor(state).to(device)\n",
    "        new_state = torch.Tensor(new_state).to(device)\n",
    "        reward = torch.Tensor(reward).to(device)\n",
    "        action = torch.LongTensor(action).to(device)\n",
    "        done = torch.Tensor(done).to(device)\n",
    "        \n",
    "        # select action : get the index associated with max q value from prediction network\n",
    "        new_state_indxs = self.dqn(new_state).detach() \n",
    "        max_new_state_indxs = torch.max(new_state_indxs, 1)[1] # to get the max new state indexes\n",
    "        # print(max_new_state_indxs)\n",
    "        \n",
    "        \n",
    "        # Using the best action from the prediction nn get the max new state value in target dqn\n",
    "        new_state_values = self.target_dqn(new_state).detach()\n",
    "        max_new_state_values = new_state_values.gather(1, max_new_state_indxs.unsqueeze(1)).squeeze(1)\n",
    "        # print(max_new_state_values)\n",
    "        \n",
    "        target_value = reward + (1 - done) * args.DISCOUNT_FACTOR * max_new_state_values #when done = 1 then target = reward\n",
    "        \n",
    "        predicted_value = self.dqn(state).gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # print()\n",
    "        # print(predicted_value,target_value)\n",
    "        loss = self.criterion(predicted_value, target_value)\n",
    "        # loss = self.criterion(target_value, predicted_value)\n",
    "        \n",
    "        # following is for logging only##################\n",
    "        loss_output = self.criterion(predicted_value, target_value)\n",
    "        loss_output_diff = predicted_value - target_value\n",
    "       ############################################### ####\n",
    "        writer.add_scalar(\"train/losses/td_loss_steps_abs\", loss_output_diff.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/td_loss_steps\", loss.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/predictedmean\", predicted_value.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/targetmean\", target_value.mean().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/predictedmin\", predicted_value.min().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/targetmin\", target_value.min().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/predictedmax\",\n",
    "                          predicted_value.max().item(), epi)\n",
    "        writer.add_scalar(\"train/losses/targetmax\", target_value.max().item(), epi)\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for tag, value in self.dqn.named_parameters():\n",
    "            writer.add_scalar(\"train/charts/gradsum \",\n",
    "                            np.array(value.grad.cpu()).sum(), epi)\n",
    "            writer.add_scalar(\"train/charts/gradmean\", np.array(value.grad.cpu()).mean(), epi)\n",
    "            writer.add_scalar(\"train/charts/gradmin\", np.array(value.grad.cpu()).min(), epi)\n",
    "            writer.add_scalar(\"train/charts/gradmax\", np.array(value.grad.cpu()).max(), epi)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if (self.target_dqn_update_counter % args.UPDATE_TARGET_FREQUENCY == 0):\n",
    "            # self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "            ## new\n",
    "            target_net_state_dict = self.target_dqn.state_dict()\n",
    "            policy_net_state_dict = self.dqn.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key] * \\\n",
    "                args.TAU + target_net_state_dict[key]*(1-args.TAU)\n",
    "            self.target_dqn.load_state_dict(target_net_state_dict)\n",
    "            \n",
    "        return (loss.mean().item())\n",
    "        \n",
    "    def save_model(self,epi,total_epi):\n",
    "        if (epi % args.MODEL_SAVE_FREQUENCY ==0):\n",
    "            torch.save(self.dqn.state_dict(),\n",
    "                       f\"runs/{'DDQN'}{args.run_name}/{'DDQN'}{args.run_name}_{'episode#'}_{epi}.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLTj1P7tnNso"
   },
   "outputs": [],
   "source": [
    "# Instantiating the DQN Agent\n",
    "torch.manual_seed(args.seed)\n",
    "dqn_agent = DQN_Agent()\n",
    "# torch.manual_seed(args.seed)\n",
    "model_dqn = dqn_agent.dqn\n",
    "# writer = SummaryWriter(f\"runs/{'DDQN'}{run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the hyperparameters to tensorboard\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\n",
    "        \"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 132682,
     "status": "ok",
     "timestamp": 1586934401261,
     "user": {
      "displayName": "Aritra Sen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiSYFenYhgXJH87pjUx-NhoqI30ZP-OIGh4O5vjpQ=s64",
      "userId": "13202125398367881278"
     },
     "user_tz": -330
    },
    "id": "QjSSkmLpnNs0",
    "outputId": "51bec1ef-f48a-49a2-ed67-53849e88fa42"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "steps_total = []\n",
    "reward_total_all_episodes = []\n",
    "steps_counter = 0\n",
    "episode_counter = []\n",
    "e_counter = 1\n",
    "episode_done = False\n",
    "loss_step_total = []\n",
    "q_pred_step_total=[]\n",
    "q_targ_step_total=[]\n",
    "for episode in range(args.NUMBER_OF_EPISODES+1):\n",
    "    print('###############Running Trainning#############################')\n",
    "    env = gym.make('Vehicle-v0') \n",
    "    \n",
    "    episode_counter.append(episode)\n",
    "    if episode == args.NUMBER_OF_EPISODES:\n",
    "        episode_done = True\n",
    "        \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    # print('episode :',e_counter)\n",
    "    LR_comp = linear_schedule(\n",
    "        args.LEARNING_RATE, args.LEARNING_RATE/20, args.NUMBER_OF_EPISODES, e_counter)\n",
    "    writer.add_scalar(\"train/learning_rate\", LR_comp, episode)\n",
    "\n",
    "    step = 0\n",
    "    reward_total = []\n",
    "    reward_app = []\n",
    "    # EGREEDY = calculate_epsilon(NUMBER_OF_EPISODES)\n",
    "    if episode<args.NUMBER_OF_EPISODES-10:\n",
    "        EGREEDY = linear_schedule(args.EGREEDY_START, args.EGREEDY_FINAL, args.NUMBER_OF_EPISODES, e_counter)\n",
    "        # LR_comp = linear_schedule(\n",
    "        #     args.LEARNING_RATE, 0.0001, args.NUMBER_OF_EPISODES, e_counter)\n",
    "    else:\n",
    "        EGREEDY = linear_schedule(\n",
    "            args.EGREEDY_START, args.EGREEDY_FINAL, args.NUMBER_OF_EPISODES, e_counter)\n",
    "        # LR_comp = linear_schedule(\n",
    "        #     args.LEARNING_RATE, 0.0001, args.NUMBER_OF_EPISODES, e_counter)\n",
    "    print('Epsilon:', EGREEDY)\n",
    "    print('Episode:',e_counter)\n",
    "   \n",
    "    for i in range(args.MAX_STEPS):\n",
    "        step += 1\n",
    "        steps_counter += 1\n",
    "        \n",
    "        [state1,state2,state3] = state\n",
    "        writer.add_scalar(\"train/states/state1\", state1, steps_counter)\n",
    "        writer.add_scalar(\"train/states/state2\", state2, steps_counter)\n",
    "        writer.add_scalar(\"train/states/state3\", state3, steps_counter)\n",
    "        \n",
    "        writer.add_scalar(\"train/state1\", state1, step)\n",
    "        writer.add_scalar(\"train/state2\", state2, step)\n",
    "        writer.add_scalar(\"train/state3\", state3, step)\n",
    "            \n",
    "        action = dqn_agent.select_action(state, EGREEDY)\n",
    "            \n",
    "        writer.add_scalar(\"train/action/action\", action, steps_counter)\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        reward_app.append(reward)\n",
    "        if(episode > args.TRAINING_STARTS):\n",
    "            loss_step = dqn_agent.optimize(steps_counter,LR_comp)\n",
    "            if loss_step == None:\n",
    "                loss_step = 0\n",
    "                q_pred_step = 0\n",
    "                q_targ_step = 0\n",
    "            loss_step_total.append(loss_step)\n",
    "            # q_pred_step_total.append(q_pred_step)\n",
    "            # q_targ_step_total.append(q_targ_step)\n",
    "            state = new_state\n",
    "            # os.system('cls')\n",
    "            # UU = ucb_agent.UCB_calc(1, 1.0, state, episode)\n",
    "            writer.add_scalar(\"train/charts/epsilon\", EGREEDY, episode)\n",
    "            if done:\n",
    "                steps_total.append(step)\n",
    "                reward_total = sum(reward_app)\n",
    "                writer.add_scalar(\"train/reward\", reward_total, episode)\n",
    "                reward_total_all_episodes.append(reward_total)\n",
    "                print('loss',sum(loss_step_total)/len(loss_step_total))\n",
    "                writer.add_scalar(\"train/losses/td_loss\",sum(loss_step_total)/len(loss_step_total), episode)\n",
    "                e_counter = e_counter+1\n",
    "                dqn_agent.save_model(episode, args.NUMBER_OF_EPISODES)\n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DQN__ExperienceReplay_TargetNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "clean_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd7a6cd272f30e4f6e264e6677d4e63cfd962276a42f91601c3e643110df28fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
